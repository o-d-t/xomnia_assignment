{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-bulgaria",
   "metadata": {},
   "source": [
    "# Raw Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_datetime(datetime_column):\n",
    "    return pd.to_datetime(datetime_column, unit = 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw messages, constructing a MultiIndex from device_id and correctly parsed datetime\n",
    "raw_data = pd.read_csv('data/raw_messages.csv', \n",
    "                       index_col=['device_id','datetime'], \n",
    "                       parse_dates=['datetime'], \n",
    "                       date_parser=parse_datetime).sort_index()\n",
    "\n",
    "# Drop duplicates.\n",
    "raw_data.drop_duplicates(inplace=True)\n",
    "# Remove non-alphanumeric noisy characters from raw_message column.\n",
    "raw_data['raw_message'] = raw_data['raw_message'].str.replace('[^a-zA-Z0-9,.]','', regex=True)\n",
    "# Expand raw message column into separate columns.\n",
    "raw_data = pd.concat([raw_data.drop('raw_message', axis=1), raw_data['raw_message'].str.split(',', expand=True)], \n",
    "                     axis=1)\n",
    "# Rename the expanded columns into correct raw message attributes.\n",
    "rename_dict = {0: 'status', \n",
    "               1: 'lat', \n",
    "               2: 'lat_dir', \n",
    "               3: 'lon', \n",
    "               4: 'lon_dir', \n",
    "               5: 'spd_over_grnd', \n",
    "               6: 'true_course', \n",
    "               7: 'datestamp', \n",
    "               8: 'mag_variation', \n",
    "               9: 'mag_var_dir'}\n",
    "raw_data.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Fix dtypes of numeric columns.\n",
    "dtypes_dict = {'lat': np.float64,\n",
    "               'lon': np.float64,\n",
    "               'spd_over_grnd': np.float64,\n",
    "               'true_course': np.float64,\n",
    "               'datestamp': np.int64,\n",
    "               'mag_variation': np.float64}\n",
    "raw_data = raw_data.astype(dtypes_dict, errors='raise').round(decimals=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print overview of raw data for st-1a2090.\n",
    "raw_data.xs('st-1a2090',level='device_id').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_raw_data = pd.read_csv('data/raw_messages_clean.csv', \n",
    "                       index_col=['device_id','datetime'], \n",
    "                       parse_dates=['datetime']).sort_index().round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print overview of clean raw data for st-1a2090.\n",
    "clean_raw_data.xs('st-1a2090',level='device_id').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-facility",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show that after rounding floats to 3 decimals, raw_data and clean_raw_data are identical.\n",
    "pd.concat([raw_data,clean_raw_data]).drop_duplicates(keep=False).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-peoples",
   "metadata": {},
   "source": [
    "# Weather Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read weather_data.json at weather stations level (without normalizing data field).\n",
    "weather_stations = pd.read_json('data/weather_data.json').rename(columns={'lat': 's_lat', \n",
    "                                                                          'lon': 's_lon'}).round(decimals=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print overview of weather stations.\n",
    "weather_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read full weather data by normalizing data field into separate columns.\n",
    "with open('data/weather_data.json') as f:\n",
    "  weather_data_json = json.load(f)\n",
    "\n",
    "weather_data= pd.json_normalize(weather_data_json,record_path='data', \n",
    "                                meta=['station_id','lat','lon']).rename(columns={'lat': 's_lat', \n",
    "                                                                                 'lon': 's_lon',\n",
    "                                                                                 'datetime': 's_datetime'})\n",
    "weather_data['timestamp_utc'] = pd.to_datetime(weather_data['timestamp_utc'])\n",
    "weather_data = weather_data.set_index('timestamp_utc').sort_index()\n",
    "dtypes_dict = {'s_lat': np.float64,\n",
    "               's_lon': np.float64}\n",
    "weather_data = weather_data.astype(dtypes_dict, errors='raise').round(decimals=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-poster",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print overview of full weather data.\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steal haversine formula implementation from internet. \n",
    "# https://medium.com/analytics-vidhya/finding-nearest-pair-of-latitude-and-longitude-match-using-python\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "def dist(lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "Replicating the same formula as mentioned in Wiki\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lat1, long1, lat2, long2 = map(radians, [lat1, long1, lat2, long2])\n",
    "    # haversine formula \n",
    "    dlon = long2 - long1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    # Radius of earth in kilometers is 6371\n",
    "    km = 6371* c\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for applying on raw data to find nearest weather station for each data point.\n",
    "def find_nearest_weather_station(lat, long):\n",
    "    distances = weather_stations.apply(lambda row: dist(lat, long, row['s_lat'], row['s_lon']), \n",
    "                                       axis=1)\n",
    "    return weather_stations.loc[distances.idxmin(), ['s_lat', 's_lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest weather station for each data point and add its coordinates to raw data.\n",
    "raw_data[['s_lat', 's_lon']] = raw_data.apply(lambda row: find_nearest_weather_station(row['lat'], row['lon']), \n",
    "                                              axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge asof on datetime indexes of raw data and weather data (nearest).\n",
    "# Merge by exact match of station lat and lon, so that data of the nearest station is grabbed.\n",
    "# Result will be a dataframe with weather data columns added on raw data.\n",
    "date = pd.to_datetime('2019-02-13')\n",
    "device_id = 'st-1a2090'\n",
    "raw_and_weather = pd.merge_asof(raw_data.xs(device_id,level='device_id').loc[date:date+pd.DateOffset(1)],\n",
    "                                weather_data,\n",
    "                                left_index = True, \n",
    "                                right_index = True, by=['s_lat','s_lon'], \n",
    "                                direction='nearest', \n",
    "                                tolerance=pd.Timedelta('1H'))\n",
    "raw_and_weather = pd.concat({device_id: raw_and_weather}, names=['device_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_and_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-headline",
   "metadata": {},
   "source": [
    "# Metric a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For how many ships do we have available data?\n",
    "raw_data.index.get_level_values('device_id').unique().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-mauritius",
   "metadata": {},
   "source": [
    "# Metric b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg speed for all available ships for each hour of the date 2019-02-13.\n",
    "date = pd.to_datetime('2019-02-13')\n",
    "raw_data.loc[pd.IndexSlice[:, date:date+pd.DateOffset(1)], :]['spd_over_grnd'].groupby([pd.Grouper(level='device_id'),\n",
    "                                                                                        pd.Grouper(level='datetime',\n",
    "                                                                                                   freq='1H')]).mean().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-tournament",
   "metadata": {},
   "source": [
    "# Metric c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max & min wind speed for every available day for ship ”st-1a2090” only.\n",
    "date = pd.to_datetime('2019-02-13')\n",
    "device_id = 'st-1a2090'\n",
    "raw_and_weather.loc[pd.IndexSlice[device_id, date:date+pd.DateOffset(1)], :]['wind_spd'].groupby([pd.Grouper(level='device_id'),\n",
    "                                                                                                  pd.Grouper(level='datetime',\n",
    "                                                                                                             freq='1H')]).max().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_and_weather.loc[pd.IndexSlice[device_id, date:date+pd.DateOffset(1)], :]['wind_spd'].groupby([pd.Grouper(level='device_id'),\n",
    "                                                                                                  pd.Grouper(level='datetime',\n",
    "                                                                                                             freq='1H')]).min().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-market",
   "metadata": {},
   "source": [
    "# Metric d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A way to visualize full weather conditions (example fields: general description, temperature, wind speed) \n",
    "# across route of the ship ”st-1a2090” for date 2019-02-13.\n",
    "date = pd.to_datetime('2019-02-13')\n",
    "device_id = 'st-1a2090'\n",
    "plot_data = raw_and_weather.loc[pd.IndexSlice[device_id, date:date+pd.DateOffset(1)], :]\n",
    "plot_data['weather_text'] = 'Timestamp: ' + plot_data.index.get_level_values(level='datetime').astype(str) + ', ' + 'Description: ' + plot_data['weather.description'].astype(str) + ', ' + 'Temperature: '+ plot_data['temp'].astype(str) + ', '+ 'Wind Speed: ' + plot_data['wind_spd'].astype(str)\n",
    "plot_data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "fig = go.Figure(data=go.Scattergeo(lon = plot_data['lon'],\n",
    "                                   lat = plot_data['lat'],\n",
    "                                   text = plot_data['weather_text'], \n",
    "                                   mode = 'lines', \n",
    "                                   line = dict(color = 'red')))\n",
    "\n",
    "fig.update_geos(fitbounds='locations',\n",
    "                resolution=50,\n",
    "                showocean=True, oceancolor=\"LightBlue\",\n",
    "                showlakes=True, lakecolor=\"Blue\",\n",
    "                showrivers=True, rivercolor=\"Blue\")\n",
    "fig.update_layout(title = f\"Route for {device_id} on {date}\" + '<br>(Hover for weather conditions)', \n",
    "                  geo_scope='europe')            \n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
